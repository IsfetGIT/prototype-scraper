


#  /$$$$$$  /$$$$$$$   /$$$$$$   /$$$$$$  /$$     /$$       /$$                                               /$$                                           /$$     /$$                    
# /$$__  $$| $$__  $$ /$$__  $$ /$$__  $$|  $$   /$$/      |__/                                              | $$                                          | $$    |__/                    
#| $$  \__/| $$  \ $$| $$  \ $$| $$  \__/ \  $$ /$$/        /$$ /$$$$$$/$$$$   /$$$$$$   /$$$$$$   /$$$$$$  /$$$$$$          /$$$$$$$  /$$$$$$   /$$$$$$$ /$$$$$$   /$$  /$$$$$$  /$$$$$$$ 
#|  $$$$$$ | $$$$$$$/| $$$$$$$$| $$        \  $$$$/        | $$| $$_  $$_  $$ /$$__  $$ /$$__  $$ /$$__  $$|_  $$_/         /$$_____/ /$$__  $$ /$$_____/|_  $$_/  | $$ /$$__  $$| $$__  $$
# \____  $$| $$____/ | $$__  $$| $$         \  $$/         | $$| $$ \ $$ \ $$| $$  \ $$| $$  \ $$| $$  \__/  | $$          |  $$$$$$ | $$$$$$$$| $$        | $$    | $$| $$  \ $$| $$  \ $$
# /$$  \ $$| $$      | $$  | $$| $$    $$    | $$          | $$| $$ | $$ | $$| $$  | $$| $$  | $$| $$        | $$ /$$       \____  $$| $$_____/| $$        | $$ /$$| $$| $$  | $$| $$  | $$
#|  $$$$$$/| $$      | $$  | $$|  $$$$$$/    | $$          | $$| $$ | $$ | $$| $$$$$$$/|  $$$$$$/| $$        |  $$$$/       /$$$$$$$/|  $$$$$$$|  $$$$$$$  |  $$$$/| $$|  $$$$$$/| $$  | $$
# \______/ |__/      |__/  |__/ \______/     |__/          |__/|__/ |__/ |__/| $$____/  \______/ |__/         \___/        |_______/  \_______/ \_______/   \___/  |__/ \______/ |__/  |__/
#                                                                            | $$                                                                                                          
#                                                                            | $$                                                                                                          
#                                                                            |__/                                                                                                          


#word tokenizer function with SPACY
import spacy
spacy.load('it')
from spacy.lang.it import Italian
parser = Italian()

def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens